{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TayyabKhan54/Machine-learning-project-/blob/main/machinelearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_L2X1QVa0fY",
        "outputId": "9c464bfb-ecb8-44be-e008-5e610d69e05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "   PCA EVALUATION ‚Äî HANDWRITTEN DIGITS DATASET\n",
            "============================================================\n",
            "\n",
            "üì¶ PART 1: DATA INSPECTION\n",
            "----------------------------------------\n",
            "  ‚úÖ Total Samples  : 1797\n",
            "  ‚úÖ Total Features : 64\n",
            "  ‚úÖ Image Size     : 8 √ó 8 pixels\n",
            "  ‚úÖ Digit Classes  : 10 (digits 0 through 9)\n",
            "  ‚úÖ Feature Range  : min=0.0, max=16.0\n",
            "\n",
            "  üíæ Saved: part1_sample_images.png\n",
            "\n",
            "üìä PART 2: PCA ANALYSIS\n",
            "----------------------------------------\n",
            "  ‚úÖ Feature scaling applied (StandardScaler: mean=0, std=1)\n",
            "     After scaling ‚Äî mean ‚âà 0.000000, std ‚âà 0.9763\n",
            "\n",
            "  Top 10 components and their explained variance:\n",
            "    PC 1: 12.03% variance | Cumulative: 12.03%\n",
            "    PC 2: 9.56% variance | Cumulative: 21.59%\n",
            "    PC 3: 8.44% variance | Cumulative: 30.04%\n",
            "    PC 4: 6.50% variance | Cumulative: 36.54%\n",
            "    PC 5: 4.86% variance | Cumulative: 41.40%\n",
            "    PC 6: 4.21% variance | Cumulative: 45.61%\n",
            "    PC 7: 3.94% variance | Cumulative: 49.55%\n",
            "    PC 8: 3.39% variance | Cumulative: 52.94%\n",
            "    PC 9: 3.00% variance | Cumulative: 55.94%\n",
            "    PC10: 2.93% variance | Cumulative: 58.87%\n",
            "\n",
            "  ‚úÖ Components for 90% variance: 31\n",
            "  ‚úÖ Components for 95% variance: 40\n",
            "     Actual cumulative variance at 31 components: 90.05%\n",
            "     Actual cumulative variance at 40 components: 95.08%\n",
            "\n",
            "  üíæ Saved: part2_pca_variance.png\n",
            "\n",
            "üìâ PART 3: DIMENSIONALITY REDUCTION\n",
            "----------------------------------------\n",
            "  Original Number of Features  : 64\n",
            "\n",
            "  [90% Variance Case]\n",
            "  Reduced Number of Features   : 31\n",
            "  Dimensionality Reduction     : 51.6%\n",
            "  Formula: (1 - 31/64) √ó 100 = 51.6%\n",
            "\n",
            "  [95% Variance Case]\n",
            "  Reduced Number of Features   : 40\n",
            "  Dimensionality Reduction     : 37.5%\n",
            "  Formula: (1 - 40/64) √ó 100 = 37.5%\n",
            "\n",
            "üîÑ PART 4: RECONSTRUCTION ERROR\n",
            "----------------------------------------\n",
            "  [90% Variance ‚Äî 31 components]\n",
            "  Reconstruction MSE  : 1.9308\n",
            "  Meaning: Each pixel is off by ‚âà 1.39 intensity units on average\n",
            "\n",
            "  [95% Variance ‚Äî 40 components]\n",
            "  Reconstruction MSE  : 1.0814\n",
            "  Meaning: Each pixel is off by ‚âà 1.04 intensity units on average\n",
            "\n",
            "  ‚úÖ Using more components (95%) reduces MSE by 44.0% compared to 90% case\n",
            "\n",
            "  üíæ Saved: part4_reconstruction.png\n",
            "\n",
            "üé® PART 5: 2D PCA VISUALIZATION\n",
            "----------------------------------------\n",
            "  PC1 explains: 12.0% variance\n",
            "  PC2 explains: 9.6% variance\n",
            "  Together (2D projection): 21.6% of total variance\n",
            "  üíæ Saved: part5_2d_visualization.png\n",
            "\n",
            "üìã Generating Summary Dashboard...\n",
            "  üíæ Saved: summary_dashboard.png\n",
            "\n",
            "============================================================\n",
            "   FINAL RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "  Dataset : Handwritten Digits (sklearn)\n",
            "  Samples : 1797     Features: 64\n",
            "\n",
            "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "  ‚îÇ  Variance  ‚îÇ Components ‚îÇ  Reduction  ‚îÇ  MSE   ‚îÇ\n",
            "  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
            "  ‚îÇ    90%     ‚îÇ     31     ‚îÇ    51.6%   ‚îÇ 1.9308 ‚îÇ\n",
            "  ‚îÇ    95%     ‚îÇ     40     ‚îÇ    37.5%   ‚îÇ 1.0814 ‚îÇ\n",
            "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "  2D Projection Variance: 21.6%\n",
            "  (PC1=12.0%, PC2=9.6%)\n",
            "\n",
            "============================================================\n",
            "   CONCLUSION\n",
            "============================================================\n",
            "\n",
            "This experiment demonstrates the effectiveness of PCA for\n",
            "dimensionality reduction on the handwritten digits dataset.\n",
            "\n",
            "The original 64-pixel features can be reduced to just\n",
            "31 components while retaining 90% of variance, and\n",
            "40 components for 95% ‚Äî reductions of 51.6% and\n",
            "37.5% respectively. This dramatically reduces storage\n",
            "and computation while preserving most information.\n",
            "\n",
            "Reconstruction error (MSE) confirms this tradeoff: the\n",
            "90% case has higher error (1.9308) while the 95% case\n",
            "achieves lower error (1.0814) with only a few more\n",
            "components. Visual inspection shows 95% reconstructions\n",
            "are nearly indistinguishable from originals.\n",
            "\n",
            "The 2D scatter plot reveals meaningful clustering ‚Äî digit\n",
            "classes form loose but distinct regions in PC1-PC2 space,\n",
            "confirming that PCA captures true structure in the data\n",
            "even without using class labels (unsupervised). Digits\n",
            "like 0 and 1 are more isolated, while 3, 5, 8 show more\n",
            "overlap due to similar stroke patterns.\n",
            "\n",
            "Overall, PCA proves to be a powerful, interpretable tool\n",
            "for compressing high-dimensional image data with minimal\n",
            "information loss and clear geometric insight.\n",
            "\n",
            "  ALL PLOTS SAVED SUCCESSFULLY ‚úÖ\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "#   PURE PCA EVALUATION ON HANDWRITTEN DIGITS DATASET\n",
        "#   Assignment | All lines are commented for easy understanding\n",
        "# ============================================================\n",
        "\n",
        "# --- Import all required libraries ---\n",
        "import numpy as np                          # NumPy for numerical operations and arrays\n",
        "import matplotlib.pyplot as plt             # Matplotlib for plotting all graphs\n",
        "import matplotlib.gridspec as gridspec      # GridSpec for custom subplot layouts\n",
        "from sklearn.datasets import load_digits    # Built-in digits dataset (8x8 images, 1797 samples)\n",
        "from sklearn.preprocessing import StandardScaler  # For feature scaling (zero mean, unit variance)\n",
        "from sklearn.decomposition import PCA       # Principal Component Analysis algorithm\n",
        "from sklearn.metrics import mean_squared_error     # MSE to measure reconstruction error\n",
        "import warnings                             # To suppress non-critical warnings\n",
        "import os                                   # For operating system interactions (e.g., creating directories)\n",
        "warnings.filterwarnings('ignore')           # Ignore all warnings for clean output\n",
        "\n",
        "# Ensure the output directory exists\n",
        "output_dir = '/mnt/user-data/outputs/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ============================================================\n",
        "#   GLOBAL STYLE SETTINGS (applied to all plots)\n",
        "# ============================================================\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'   # Set clean, readable font for all plots\n",
        "plt.rcParams['font.size'] = 10                # Base font size for labels and text\n",
        "plt.rcParams['axes.titlesize'] = 13          # Font size for subplot/chart titles\n",
        "plt.rcParams['axes.labelsize'] = 11          # Font size for axis labels (x and y)\n",
        "plt.rcParams['figure.facecolor'] = '#0f0f1a' # Dark navy background for all figures\n",
        "plt.rcParams['axes.facecolor'] = '#1a1a2e'  # Slightly lighter background inside axes\n",
        "plt.rcParams['axes.edgecolor'] = '#444466'  # Border color around each plot area\n",
        "plt.rcParams['text.color'] = '#e0e0ff'      # Light purple-white for all text\n",
        "plt.rcParams['axes.labelcolor'] = '#e0e0ff' # Same color for axis labels\n",
        "plt.rcParams['xtick.color'] = '#aaaacc'     # Color for x-axis tick marks and labels\n",
        "plt.rcParams['ytick.color'] = '#aaaacc'     # Color for y-axis tick marks and labels\n",
        "plt.rcParams['grid.color'] = '#2a2a4a'      # Subtle grid line color\n",
        "plt.rcParams['grid.linestyle'] = '--'       # Dashed grid lines\n",
        "plt.rcParams['grid.alpha'] = 0.7            # Slightly transparent grid lines\n",
        "\n",
        "# Define a reusable color palette for consistent visuals across all plots\n",
        "COLORS = {\n",
        "    'accent1': '#7b5ea7',   # Purple ‚Äî used for primary bars and lines\n",
        "    'accent2': '#00c9b1',   # Teal ‚Äî used for secondary/comparison lines\n",
        "    'accent3': '#ff6b6b',   # Coral red ‚Äî used for highlights and thresholds\n",
        "    'accent4': '#ffd166',   # Gold ‚Äî used for annotations and markers\n",
        "    'scatter': [            # 10 distinct colors, one per digit class (0-9)\n",
        "        '#ff6b6b', '#ffd166', '#06d6a0', '#118ab2', '#a8dadc',\n",
        "        '#e63946', '#f4a261', '#2a9d8f', '#e9c46a', '#c77dff'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"   PCA EVALUATION ‚Äî HANDWRITTEN DIGITS DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "#   PART 1: DATA INSPECTION\n",
        "# ============================================================\n",
        "print(\"\\nüì¶ PART 1: DATA INSPECTION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Load the digits dataset from scikit-learn\n",
        "# This dataset contains 1797 images of handwritten digits (0‚Äì9)\n",
        "# Each image is 8x8 pixels = 64 pixel intensity values as features\n",
        "digits = load_digits()\n",
        "\n",
        "# Extract the feature matrix X (shape: 1797 samples √ó 64 features)\n",
        "# Each row is one image, each column is one pixel intensity value (0‚Äì16)\n",
        "X = digits.data\n",
        "\n",
        "# Extract the target vector y (shape: 1797,)\n",
        "# Each value is the true digit label: 0, 1, 2, ..., 9\n",
        "y = digits.target\n",
        "\n",
        "# Extract the raw images in 2D form for visualization (shape: 1797 √ó 8 √ó 8)\n",
        "images = digits.images\n",
        "\n",
        "# Report dataset dimensions to the user\n",
        "total_samples = X.shape[0]   # Number of rows = number of digit samples\n",
        "total_features = X.shape[1]  # Number of columns = number of pixel features (64)\n",
        "print(f\"  ‚úÖ Total Samples  : {total_samples}\")\n",
        "print(f\"  ‚úÖ Total Features : {total_features}\")\n",
        "print(f\"  ‚úÖ Image Size     : 8 √ó 8 pixels\")\n",
        "print(f\"  ‚úÖ Digit Classes  : {len(np.unique(y))} (digits 0 through 9)\")\n",
        "print(f\"  ‚úÖ Feature Range  : min={X.min():.1f}, max={X.max():.1f}\")\n",
        "\n",
        "# ---- Plot: Display 10 sample digit images (one per digit class) ----\n",
        "fig1, axes = plt.subplots(2, 5, figsize=(12, 5.5))  # 2 rows √ó 5 columns = 10 subplots\n",
        "fig1.patch.set_facecolor('#0f0f1a')                  # Set figure background to dark\n",
        "fig1.suptitle('PART 1 ‚Äî Sample Digit Images (one per class 0‚Äì9)',\n",
        "              fontsize=14, color='#e0e0ff',          # Title text color\n",
        "              fontweight='bold', y=1.02)             # Position title slightly above\n",
        "\n",
        "# Loop over each digit class (0 through 9) and display one example image\n",
        "for digit_class in range(10):\n",
        "    # Find the index of the first image that matches this digit class\n",
        "    first_index = np.where(y == digit_class)[0][0]\n",
        "\n",
        "    # Calculate subplot row (0 or 1) and column (0‚Äì4) position\n",
        "    row = digit_class // 5   # Integer division: 0-4 ‚Üí row 0, 5-9 ‚Üí row 1\n",
        "    col = digit_class % 5    # Remainder: cycles 0,1,2,3,4 for both rows\n",
        "\n",
        "    ax = axes[row, col]                          # Select the specific subplot\n",
        "    ax.imshow(images[first_index],               # Display the 8√ó8 image array\n",
        "              cmap='plasma',                     # Colormap: purple-to-yellow gradient\n",
        "              interpolation='nearest')           # No blurring between pixels\n",
        "    ax.set_title(f'Digit: {digit_class}',        # Label the subplot with the digit\n",
        "                 color='#ffd166',                # Gold color for digit titles\n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.axis('off')                               # Remove x/y axis ticks and labels\n",
        "\n",
        "plt.tight_layout()              # Prevent overlapping between subplots\n",
        "plt.savefig('/mnt/user-data/outputs/part1_sample_images.png',\n",
        "            dpi=150, bbox_inches='tight',        # Crop whitespace around figure\n",
        "            facecolor='#0f0f1a')                 # Preserve dark background in saved file\n",
        "plt.close()                                      # Close figure to free memory\n",
        "print(\"\\n  üíæ Saved: part1_sample_images.png\")\n",
        "\n",
        "# ============================================================\n",
        "#   PART 2: PCA ANALYSIS\n",
        "# ============================================================\n",
        "print(\"\\nüìä PART 2: PCA ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# ---- Step 2A: Feature Scaling ----\n",
        "# StandardScaler transforms data so each feature has:\n",
        "#   mean = 0 (centered) and standard deviation = 1 (unit variance)\n",
        "# This is critical before PCA because PCA is sensitive to feature scales.\n",
        "# Without scaling, high-intensity pixels would dominate the principal components.\n",
        "scaler = StandardScaler()          # Create the scaler object\n",
        "\n",
        "# fit_transform computes mean & std from X, then transforms X in one step\n",
        "# The result X_scaled has the same shape as X (1797 √ó 64) but normalized values\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"  ‚úÖ Feature scaling applied (StandardScaler: mean=0, std=1)\")\n",
        "print(f\"     After scaling ‚Äî mean ‚âà {X_scaled.mean():.6f}, std ‚âà {X_scaled.std():.4f}\")\n",
        "\n",
        "# ---- Step 2B: Apply PCA with all 64 components ----\n",
        "# We first run PCA with ALL components to analyze the full variance structure.\n",
        "# n_components=64 means we keep all principal components (same as number of features).\n",
        "# random_state=42 ensures reproducibility of results.\n",
        "pca_full = PCA(n_components=64, random_state=42)\n",
        "\n",
        "# fit_transform projects the scaled data into PCA space (still 64 dimensions here)\n",
        "X_pca_full = pca_full.fit_transform(X_scaled)\n",
        "\n",
        "# ---- Step 2C: Extract Explained Variance Ratio ----\n",
        "# explained_variance_ratio_ is an array of 64 values.\n",
        "# Each value tells us what fraction of total variance is captured by that component.\n",
        "# Example: [0.12, 0.097, 0.085, ...] means PC1 explains 12%, PC2 explains 9.7%, etc.\n",
        "evr = pca_full.explained_variance_ratio_        # Array of 64 variance ratios\n",
        "\n",
        "# Cumulative sum: adds up variance ratios progressively\n",
        "# cumvar[i] = total variance explained by the first (i+1) principal components\n",
        "cumvar = np.cumsum(evr)                          # Array of 64 cumulative variance values\n",
        "\n",
        "print(\"\\n  Top 10 components and their explained variance:\")\n",
        "for i in range(10):       # Print information for the first 10 principal components\n",
        "    print(f\"    PC{i+1:2d}: {evr[i]*100:.2f}% variance | Cumulative: {cumvar[i]*100:.2f}%\")\n",
        "\n",
        "# ---- Step 2D: Determine Minimum Components for 90% and 95% Variance ----\n",
        "# np.argmax returns the index of the FIRST position where condition is True.\n",
        "# cumvar >= 0.90 is a boolean array; argmax finds first True index.\n",
        "# We add 1 because components are 1-indexed (PC1, PC2, ...).\n",
        "n_components_90 = np.argmax(cumvar >= 0.90) + 1  # Min components for 90% variance\n",
        "n_components_95 = np.argmax(cumvar >= 0.95) + 1  # Min components for 95% variance\n",
        "\n",
        "print(f\"\\n  ‚úÖ Components for 90% variance: {n_components_90}\")\n",
        "print(f\"  ‚úÖ Components for 95% variance: {n_components_95}\")\n",
        "print(f\"     Actual cumulative variance at {n_components_90} components: {cumvar[n_components_90-1]*100:.2f}%\")\n",
        "print(f\"     Actual cumulative variance at {n_components_95} components: {cumvar[n_components_95-1]*100:.2f}%\")\n",
        "\n",
        "# ---- Plot: Scree Plot + Cumulative Variance Plot (side by side) ----\n",
        "fig2, (ax_scree, ax_cum) = plt.subplots(1, 2, figsize=(14, 5.5))  # Two plots side by side\n",
        "fig2.patch.set_facecolor('#0f0f1a')  # Dark figure background\n",
        "fig2.suptitle('PART 2 ‚Äî PCA Variance Analysis',\n",
        "              fontsize=14, color='#e0e0ff', fontweight='bold', y=1.02)\n",
        "\n",
        "# --- Left plot: Scree Plot ---\n",
        "# A scree plot shows individual explained variance for each component.\n",
        "# The \"elbow\" point indicates where adding more components gives diminishing returns.\n",
        "component_numbers = np.arange(1, 65)       # Component labels: 1, 2, 3, ..., 64\n",
        "\n",
        "ax_scree.bar(component_numbers,            # x-axis: component index\n",
        "             evr * 100,                    # y-axis: individual variance % for each\n",
        "             color=COLORS['accent1'],      # Purple bars\n",
        "             alpha=0.85,                   # Slightly transparent\n",
        "             edgecolor='#5a3e8a',          # Darker purple edges on bars\n",
        "             linewidth=0.5)                # Thin edges\n",
        "\n",
        "# Add a smooth line on top of the bar chart to show the trend\n",
        "ax_scree.plot(component_numbers, evr * 100,\n",
        "              color=COLORS['accent2'],     # Teal line for visual contrast\n",
        "              linewidth=1.5,              # Moderate line thickness\n",
        "              marker='o',                 # Circular markers at each data point\n",
        "              markersize=2,               # Small markers to avoid clutter\n",
        "              alpha=0.8)                  # Slight transparency\n",
        "\n",
        "ax_scree.set_title('Scree Plot', color='#e0e0ff', pad=10)\n",
        "ax_scree.set_xlabel('Principal Component Number')\n",
        "ax_scree.set_ylabel('Explained Variance (%)')\n",
        "ax_scree.grid(True, alpha=0.4)            # Show subtle grid lines\n",
        "\n",
        "# Mark the elbow region (around component 10) with a vertical dashed line\n",
        "ax_scree.axvline(x=10, color=COLORS['accent4'], linestyle='--',\n",
        "                 linewidth=1.2, alpha=0.8, label='Elbow ‚âà PC10')\n",
        "ax_scree.legend(fontsize=9, facecolor='#1a1a2e', labelcolor='#e0e0ff')\n",
        "\n",
        "# Annotate the first few high-variance components with their values\n",
        "for i in [0, 1, 2]:     # Annotate PC1, PC2, PC3 with their variance %\n",
        "    ax_scree.annotate(f'{evr[i]*100:.1f}%',\n",
        "                      xy=(i+1, evr[i]*100),       # Arrow points to the bar top\n",
        "                      xytext=(i+3, evr[i]*100+0.5),  # Text slightly offset\n",
        "                      fontsize=8, color=COLORS['accent4'],\n",
        "                      arrowprops=dict(arrowstyle='->', color=COLORS['accent4'], lw=1.2))\n",
        "\n",
        "# --- Right plot: Cumulative Explained Variance ---\n",
        "# This plot shows how total captured variance increases as we add more components.\n",
        "# Helps us decide the minimum number of components needed to retain a target %.\n",
        "ax_cum.plot(component_numbers, cumvar * 100,\n",
        "            color=COLORS['accent2'],       # Teal line for cumulative variance\n",
        "            linewidth=2.5,                # Thicker line for emphasis\n",
        "            marker='o', markersize=2)\n",
        "\n",
        "# Shade the area under the cumulative variance curve for visual impact\n",
        "ax_cum.fill_between(component_numbers, cumvar * 100,\n",
        "                    alpha=0.15,            # Very transparent fill\n",
        "                    color=COLORS['accent2'])\n",
        "\n",
        "# Draw a horizontal dashed line at the 90% variance threshold\n",
        "ax_cum.axhline(y=90, color=COLORS['accent3'], linestyle='--',\n",
        "               linewidth=1.5, label=f'90% ({n_components_90} components)')\n",
        "\n",
        "# Draw a horizontal dashed line at the 95% variance threshold\n",
        "ax_cum.axhline(y=95, color=COLORS['accent4'], linestyle='--',\n",
        "               linewidth=1.5, label=f'95% ({n_components_95} components)')\n",
        "\n",
        "# Draw a vertical dashed line at the 90% component threshold\n",
        "ax_cum.axvline(x=n_components_90, color=COLORS['accent3'],\n",
        "               linestyle=':', linewidth=1.2, alpha=0.8)\n",
        "\n",
        "# Draw a vertical dashed line at the 95% component threshold\n",
        "ax_cum.axvline(x=n_components_95, color=COLORS['accent4'],\n",
        "               linestyle=':', linewidth=1.2, alpha=0.8)\n",
        "\n",
        "# Add text annotations showing exactly where 90% and 95% are reached\n",
        "ax_cum.annotate(f'PC{n_components_90}‚Üí90%',\n",
        "                xy=(n_components_90, 90),\n",
        "                xytext=(n_components_90 + 5, 80),\n",
        "                color=COLORS['accent3'], fontsize=9,\n",
        "                arrowprops=dict(arrowstyle='->', color=COLORS['accent3'], lw=1.2))\n",
        "\n",
        "ax_cum.annotate(f'PC{n_components_95}‚Üí95%',\n",
        "                xy=(n_components_95, 95),\n",
        "                xytext=(n_components_95 + 5, 85),\n",
        "                color=COLORS['accent4'], fontsize=9,\n",
        "                arrowprops=dict(arrowstyle='->', color=COLORS['accent4'], lw=1.2))\n",
        "\n",
        "ax_cum.set_title('Cumulative Explained Variance', color='#e0e0ff', pad=10)\n",
        "ax_cum.set_xlabel('Number of Principal Components')\n",
        "ax_cum.set_ylabel('Cumulative Explained Variance (%)')\n",
        "ax_cum.set_ylim([0, 102])            # Set y-axis from 0% to slightly above 100%\n",
        "ax_cum.grid(True, alpha=0.4)\n",
        "ax_cum.legend(fontsize=9, facecolor='#1a1a2e', labelcolor='#e0e0ff')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/mnt/user-data/outputs/part2_pca_variance.png',\n",
        "            dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
        "plt.close()\n",
        "print(\"\\n  üíæ Saved: part2_pca_variance.png\")\n",
        "\n",
        "# ============================================================\n",
        "#   PART 3: DIMENSIONALITY REDUCTION\n",
        "# ============================================================\n",
        "print(\"\\nüìâ PART 3: DIMENSIONALITY REDUCTION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# We now apply PCA with ONLY the number of components needed for 95% variance.\n",
        "# This reduces the 64-dimensional data to n_components_95 dimensions.\n",
        "# We use 95% as the primary reduction target for highest quality reconstruction.\n",
        "pca_95 = PCA(n_components=n_components_95, random_state=42)\n",
        "\n",
        "# Transform the scaled data into the reduced PCA space\n",
        "# X_reduced_95 has shape (1797, n_components_95) ‚Äî much smaller than (1797, 64)\n",
        "X_reduced_95 = pca_95.fit_transform(X_scaled)\n",
        "\n",
        "# Also apply PCA for the 90% variance case (for comparison)\n",
        "pca_90 = PCA(n_components=n_components_90, random_state=42)\n",
        "X_reduced_90 = pca_90.fit_transform(X_scaled)  # Shape: (1797, n_components_90)\n",
        "\n",
        "# Compute how much we reduced the dimensionality compared to original 64 features\n",
        "original_features = X.shape[1]      # Original feature count = 64\n",
        "\n",
        "# Reduction % = ((original - reduced) / original) √ó 100\n",
        "reduction_pct_95 = (1 - n_components_95 / original_features) * 100  # For 95% variance case\n",
        "reduction_pct_90 = (1 - n_components_90 / original_features) * 100  # For 90% variance case\n",
        "\n",
        "print(f\"  Original Number of Features  : {original_features}\")\n",
        "print(f\"\\n  [90% Variance Case]\")\n",
        "print(f\"  Reduced Number of Features   : {n_components_90}\")\n",
        "print(f\"  Dimensionality Reduction     : {reduction_pct_90:.1f}%\")\n",
        "print(f\"  Formula: (1 - {n_components_90}/{original_features}) √ó 100 = {reduction_pct_90:.1f}%\")\n",
        "\n",
        "print(f\"\\n  [95% Variance Case]\")\n",
        "print(f\"  Reduced Number of Features   : {n_components_95}\")\n",
        "print(f\"  Dimensionality Reduction     : {reduction_pct_95:.1f}%\")\n",
        "print(f\"  Formula: (1 - {n_components_95}/{original_features}) √ó 100 = {reduction_pct_95:.1f}%\")\n",
        "\n",
        "# ============================================================\n",
        "#   PART 4: RECONSTRUCTION ERROR\n",
        "# ============================================================\n",
        "print(\"\\nüîÑ PART 4: RECONSTRUCTION ERROR\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# PCA Reconstruction: We can convert back from reduced space to original 64D space.\n",
        "# inverse_transform reverses the PCA projection using the stored components.\n",
        "# The reconstruction is an APPROXIMATION ‚Äî some information is lost (the discarded variance).\n",
        "X_reconstructed_95 = pca_95.inverse_transform(X_reduced_95)  # 95% case: shape (1797, 64)\n",
        "X_reconstructed_90 = pca_90.inverse_transform(X_reduced_90)  # 90% case: shape (1797, 64)\n",
        "\n",
        "# The reconstructions are in scaled space; we un-scale them back to pixel intensities\n",
        "# inverse_transform on the scaler reverses the normalization (re-adds mean, re-scales by std)\n",
        "X_reconstructed_95_orig = scaler.inverse_transform(X_reconstructed_95)\n",
        "X_reconstructed_90_orig = scaler.inverse_transform(X_reconstructed_90)\n",
        "\n",
        "# Compute Mean Squared Error (MSE) between original and reconstructed data\n",
        "# MSE = average of squared differences pixel-by-pixel across all images\n",
        "mse_95 = mean_squared_error(X, X_reconstructed_95_orig)  # MSE for 95% case\n",
        "mse_90 = mean_squared_error(X, X_reconstructed_90_orig)  # MSE for 90% case\n",
        "\n",
        "print(f\"  [90% Variance ‚Äî {n_components_90} components]\")\n",
        "print(f\"  Reconstruction MSE  : {mse_90:.4f}\")\n",
        "print(f\"  Meaning: Each pixel is off by ‚âà {np.sqrt(mse_90):.2f} intensity units on average\")\n",
        "\n",
        "print(f\"\\n  [95% Variance ‚Äî {n_components_95} components]\")\n",
        "print(f\"  Reconstruction MSE  : {mse_95:.4f}\")\n",
        "print(f\"  Meaning: Each pixel is off by ‚âà {np.sqrt(mse_95):.2f} intensity units on average\")\n",
        "\n",
        "print(f\"\\n  ‚úÖ Using more components (95%) reduces MSE by \"\n",
        "      f\"{( (mse_90 - mse_95)/mse_90)*100:.1f}% compared to 90% case\")\n",
        "\n",
        "# ---- Plot: Original vs Reconstructed Digit Images Comparison ----\n",
        "fig4 = plt.figure(figsize=(14, 7))    # Create wide figure for comparison\n",
        "fig4.patch.set_facecolor('#0f0f1a')\n",
        "fig4.suptitle('PART 4 ‚Äî Original vs Reconstructed Digit Images',\n",
        "              fontsize=14, color='#e0e0ff', fontweight='bold', y=1.01)\n",
        "\n",
        "# Select 6 diverse digit indices to compare: one from each class 0‚Äì5\n",
        "sample_indices = [np.where(y == d)[0][0] for d in range(6)]  # First sample of digit 0‚Äì5\n",
        "\n",
        "n_samples_show = len(sample_indices)   # Number of images to compare = 6\n",
        "\n",
        "# We'll show 3 rows: Original | 90% Recon | 95% Recon\n",
        "# And n_samples_show columns (one per digit)\n",
        "row_labels = ['Original', f'90% Recon\\n({n_components_90} PCs)', f'95% Recon\\n({n_components_95} PCs)']\n",
        "row_data = [X, X_reconstructed_90_orig, X_reconstructed_95_orig]  # Data for each row\n",
        "\n",
        "# Define border colors for each row to visually distinguish them\n",
        "row_border_colors = ['#7b5ea7', '#ff6b6b', '#00c9b1']   # Purple, Red, Teal\n",
        "\n",
        "for row_idx, (data, label, border_col) in enumerate(zip(row_data, row_labels, row_border_colors)):\n",
        "    for col_idx, sample_idx in enumerate(sample_indices):\n",
        "        # Calculate the position in a (3 rows √ó 6 cols) grid of subplots\n",
        "        # Subplot numbering: (row_idx * n_samples_show + col_idx + 1)\n",
        "        ax = fig4.add_subplot(3, n_samples_show, row_idx * n_samples_show + col_idx + 1)\n",
        "\n",
        "        # Reshape the flat 64-element vector back to 8√ó8 image for display\n",
        "        img = data[sample_idx].reshape(8, 8)\n",
        "\n",
        "        ax.imshow(img,              # Show the image\n",
        "                  cmap='plasma',    # Same colormap as Part 1 for consistency\n",
        "                  interpolation='nearest',\n",
        "                  vmin=0, vmax=16)  # Fix color scale to digit dataset range (0‚Äì16)\n",
        "\n",
        "        # Add row label only on the leftmost column\n",
        "        if col_idx == 0:\n",
        "            ax.set_ylabel(label, fontsize=9, color=border_col, fontweight='bold')\n",
        "\n",
        "        # Add digit number as title on the top row only\n",
        "        if row_idx == 0:\n",
        "            ax.set_title(f'Digit {y[sample_idx]}',\n",
        "                         fontsize=10, color='#ffd166', fontweight='bold')\n",
        "\n",
        "        ax.axis('off')  # Remove axis ticks and labels for cleaner look\n",
        "\n",
        "        # Draw a colored border around each subplot to match the row's theme\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_edgecolor(border_col)\n",
        "            spine.set_linewidth(2)\n",
        "            spine.set_visible(True)\n",
        "\n",
        "# Add MSE annotations as text below the comparison figure\n",
        "fig4.text(0.5, -0.03,\n",
        "          f'MSE (90%): {mse_90:.4f}    |    MSE (95%): {mse_95:.4f}    '\n",
        "          f'|    Lower MSE = Better Reconstruction',\n",
        "          ha='center', va='center', fontsize=10,\n",
        "          color='#e0e0ff', style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/mnt/user-data/outputs/part4_reconstruction.png',\n",
        "            dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
        "plt.close()\n",
        "print(\"\\n  üíæ Saved: part4_reconstruction.png\")\n",
        "\n",
        "# ============================================================\n",
        "#   PART 5: 2D VISUALIZATION (PC1 vs PC2)\n",
        "# ============================================================\n",
        "print(\"\\nüé® PART 5: 2D PCA VISUALIZATION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# For 2D visualization, apply PCA keeping only the first 2 principal components.\n",
        "# These 2 components capture the most variance and can be plotted on a 2D scatter plot.\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "\n",
        "# Project all 1797 samples into 2D space\n",
        "# X_2d has shape (1797, 2) ‚Äî each row is (PC1_value, PC2_value) for one image\n",
        "X_2d = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "# How much total variance do PC1 and PC2 together capture?\n",
        "var_pc1 = pca_2d.explained_variance_ratio_[0] * 100   # PC1 variance %\n",
        "var_pc2 = pca_2d.explained_variance_ratio_[1] * 100   # PC2 variance %\n",
        "total_2d_var = var_pc1 + var_pc2                       # Combined variance %\n",
        "\n",
        "print(f\"  PC1 explains: {var_pc1:.1f}% variance\")\n",
        "print(f\"  PC2 explains: {var_pc2:.1f}% variance\")\n",
        "print(f\"  Together (2D projection): {total_2d_var:.1f}% of total variance\")\n",
        "\n",
        "# ---- Plot: 2D Scatter Plot (PC1 vs PC2), colored by digit class ----\n",
        "fig5, ax = plt.subplots(figsize=(11, 8))    # Single large scatter plot\n",
        "fig5.patch.set_facecolor('#0f0f1a')\n",
        "\n",
        "# Plot each digit class as a separate scatter series for distinct colors and legend\n",
        "for digit_class in range(10):                          # Loop over all 10 digit classes\n",
        "    # Get boolean mask: True where y matches this digit class\n",
        "    mask = y == digit_class\n",
        "\n",
        "    ax.scatter(\n",
        "        X_2d[mask, 0],                 # PC1 values for this digit class (x-axis)\n",
        "        X_2d[mask, 1],                 # PC2 values for this digit class (y-axis)\n",
        "        c=COLORS['scatter'][digit_class],  # Unique color for each digit\n",
        "        label=f'Digit {digit_class}',  # Legend entry for this class\n",
        "        alpha=0.65,                    # Semi-transparent so overlapping points are visible\n",
        "        s=22,                          # Marker size (in points¬≤)\n",
        "        edgecolors='none'              # No border on markers for cleaner look\n",
        "    )\n",
        "\n",
        "# Add axis labels with variance percentage to show what each axis represents\n",
        "ax.set_xlabel(f'Principal Component 1  ({var_pc1:.1f}% variance)', fontsize=12)\n",
        "ax.set_ylabel(f'Principal Component 2  ({var_pc2:.1f}% variance)', fontsize=12)\n",
        "ax.set_title(f'2D PCA Projection of Handwritten Digits\\n'\n",
        "             f'(PC1+PC2 capture {total_2d_var:.1f}% of total variance)',\n",
        "             color='#e0e0ff', fontsize=13, fontweight='bold', pad=14)\n",
        "\n",
        "# Add a legend explaining which color corresponds to which digit\n",
        "legend = ax.legend(title='Digit Class',\n",
        "                   bbox_to_anchor=(1.02, 1),  # Place legend outside plot on the right\n",
        "                   loc='upper left',\n",
        "                   fontsize=9,\n",
        "                   title_fontsize=10,\n",
        "                   facecolor='#1a1a2e',       # Dark legend background\n",
        "                   edgecolor='#444466',       # Border of the legend box\n",
        "                   labelcolor='#e0e0ff')      # Text color in legend\n",
        "legend.get_title().set_color('#e0e0ff')       # Legend title color\n",
        "\n",
        "ax.grid(True, alpha=0.3)  # Add subtle grid for easier coordinate reading\n",
        "\n",
        "# Add an annotation box summarizing key stats on the plot\n",
        "info_text = (f\"Samples: {total_samples}\\n\"\n",
        "             f\"Features: {original_features} ‚Üí 2\\n\"\n",
        "             f\"Variance: {total_2d_var:.1f}%\")\n",
        "ax.text(0.02, 0.97, info_text,\n",
        "        transform=ax.transAxes,     # Coordinates relative to axes (0-1 range)\n",
        "        fontsize=9, color='#e0e0ff',\n",
        "        verticalalignment='top',\n",
        "        bbox=dict(boxstyle='round,pad=0.5',   # Rounded rectangle box\n",
        "                  facecolor='#2a2a4a',\n",
        "                  edgecolor='#7b5ea7',\n",
        "                  alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/mnt/user-data/outputs/part5_2d_visualization.png',\n",
        "            dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
        "plt.close()\n",
        "print(\"  üíæ Saved: part5_2d_visualization.png\")\n",
        "\n",
        "# ============================================================\n",
        "#   COMBINED SUMMARY DASHBOARD (all results at a glance)\n",
        "# ============================================================\n",
        "print(\"\\nüìã Generating Summary Dashboard...\")\n",
        "\n",
        "fig_sum = plt.figure(figsize=(16, 9))     # Large dashboard figure\n",
        "fig_sum.patch.set_facecolor('#0f0f1a')\n",
        "fig_sum.suptitle('PCA EVALUATION ‚Äî HANDWRITTEN DIGITS | SUMMARY DASHBOARD',\n",
        "                 fontsize=15, color='#e0e0ff', fontweight='bold', y=1.0)\n",
        "\n",
        "# Use GridSpec for flexible subplot sizing\n",
        "gs = gridspec.GridSpec(2, 3, figure=fig_sum,  # 2 rows, 3 columns\n",
        "                       hspace=0.45,           # Vertical spacing between rows\n",
        "                       wspace=0.35)           # Horizontal spacing between columns\n",
        "\n",
        "# ---- Panel 1 (top-left): Scree Plot ----\n",
        "ax1 = fig_sum.add_subplot(gs[0, 0])\n",
        "ax1.bar(component_numbers[:20], evr[:20]*100, color=COLORS['accent1'], alpha=0.85)\n",
        "ax1.plot(component_numbers[:20], evr[:20]*100, color=COLORS['accent2'], lw=1.5, marker='o', ms=3)\n",
        "ax1.set_title('Scree Plot (Top 20 PCs)', pad=8)\n",
        "ax1.set_xlabel('Component')\n",
        "ax1.set_ylabel('Variance (%)')\n",
        "ax1.grid(True, alpha=0.4)\n",
        "\n",
        "# ---- Panel 2 (top-center): Cumulative Variance ----\n",
        "ax2 = fig_sum.add_subplot(gs[0, 1])\n",
        "ax2.plot(component_numbers, cumvar*100, color=COLORS['accent2'], lw=2)\n",
        "ax2.fill_between(component_numbers, cumvar*100, alpha=0.12, color=COLORS['accent2'])\n",
        "ax2.axhline(90, color=COLORS['accent3'], ls='--', lw=1.5, label=f'90% ‚Üí {n_components_90} PCs')\n",
        "ax2.axhline(95, color=COLORS['accent4'], ls='--', lw=1.5, label=f'95% ‚Üí {n_components_95} PCs')\n",
        "ax2.set_title('Cumulative Variance', pad=8)\n",
        "ax2.set_xlabel('Components')\n",
        "ax2.set_ylabel('Cumulative (%)')\n",
        "ax2.set_ylim([0, 102])\n",
        "ax2.legend(fontsize=8, facecolor='#1a1a2e', labelcolor='#e0e0ff')\n",
        "ax2.grid(True, alpha=0.4)\n",
        "\n",
        "# ---- Panel 3 (top-right): Dimensionality Reduction Bar Chart ----\n",
        "ax3 = fig_sum.add_subplot(gs[0, 2])\n",
        "cases = ['Original\\n(64 dims)', f'90% Var\\n({n_components_90} dims)', f'95% Var\\n({n_components_95} dims)']\n",
        "dims = [64, n_components_90, n_components_95]\n",
        "bar_colors = [COLORS['accent1'], COLORS['accent3'], COLORS['accent4']]\n",
        "bars = ax3.bar(cases, dims, color=bar_colors, alpha=0.85, width=0.5, edgecolor='#2a2a3a')\n",
        "# Add value labels on top of each bar\n",
        "for bar, dim in zip(bars, dims):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             str(dim), ha='center', va='bottom', fontsize=10, color='#e0e0ff', fontweight='bold')\n",
        "ax3.set_title('Dimensionality Comparison', pad=8)\n",
        "ax3.set_ylabel('Number of Features')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# ---- Panel 4 (bottom-left): 2D Scatter Plot ----\n",
        "ax4 = fig_sum.add_subplot(gs[1, 0])\n",
        "for digit_class in range(10):\n",
        "    mask = y == digit_class\n",
        "    ax4.scatter(X_2d[mask, 0], X_2d[mask, 1],\n",
        "                c=COLORS['scatter'][digit_class],\n",
        "                label=f'{digit_class}', alpha=0.5, s=8, edgecolors='none')\n",
        "ax4.set_title(f'2D PCA Scatter ({total_2d_var:.1f}% var)', pad=8)\n",
        "ax4.set_xlabel(f'PC1 ({var_pc1:.1f}%)')\n",
        "ax4.set_ylabel(f'PC2 ({var_pc2:.1f}%)')\n",
        "ax4.legend(title='Digit', fontsize=6, title_fontsize=7,\n",
        "           facecolor='#1a1a2e', labelcolor='#e0e0ff',\n",
        "           ncol=5, loc='upper right', markerscale=2)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# ---- Panel 5 (bottom-center): Reconstruction Error Bar Chart ----\n",
        "ax5 = fig_sum.add_subplot(gs[1, 1])\n",
        "cases_mse = [f'90% Var\\n({n_components_90} PCs)', f'95% Var\\n({n_components_95} PCs)']\n",
        "mse_values = [mse_90, mse_95]\n",
        "mse_bar_colors = [COLORS['accent3'], COLORS['accent4']]\n",
        "bars5 = ax5.bar(cases_mse, mse_values, color=mse_bar_colors, alpha=0.85,\n",
        "                width=0.4, edgecolor='#2a2a3a')\n",
        "for bar, val in zip(bars5, mse_values):\n",
        "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "             f'{val:.4f}', ha='center', va='bottom', fontsize=10,\n",
        "             color='#e0e0ff', fontweight='bold')\n",
        "ax5.set_title('Reconstruction Error (MSE)', pad=8)\n",
        "ax5.set_ylabel('Mean Squared Error')\n",
        "ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# ---- Panel 6 (bottom-right): Key Results Text Summary ----\n",
        "ax6 = fig_sum.add_subplot(gs[1, 2])\n",
        "ax6.axis('off')   # This panel is purely text ‚Äî no axes needed\n",
        "\n",
        "# Build the summary text block with all key results\n",
        "summary = (\n",
        "    \"KEY RESULTS\\n\"\n",
        "    \"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\"\n",
        "    f\"Dataset: 1797 samples\\n\"\n",
        "    f\"Original features: 64\\n\\n\"\n",
        "    f\"90% Variance:\\n\"\n",
        "    f\"  Components: {n_components_90}\\n\"\n",
        "    f\"  Reduced by: {reduction_pct_90:.1f}%\\n\"\n",
        "    f\"  MSE: {mse_90:.4f}\\n\\n\"\n",
        "    f\"95% Variance:\\n\"\n",
        "    f\"  Components: {n_components_95}\\n\"\n",
        "    f\"  Reduced by: {reduction_pct_95:.1f}%\\n\"\n",
        "    f\"  MSE: {mse_95:.4f}\\n\\n\"\n",
        "    f\"2D Projection:\\n\"\n",
        "    f\"  PC1: {var_pc1:.1f}%\\n\"\n",
        "    f\"  PC2: {var_pc2:.1f}%\\n\"\n",
        "    f\"  Total: {total_2d_var:.1f}%\"\n",
        ")\n",
        "\n",
        "# Place the text block inside the empty subplot with a styled box\n",
        "ax6.text(0.05, 0.95, summary,\n",
        "         transform=ax6.transAxes,\n",
        "         fontsize=9.5, color='#e0e0ff',\n",
        "         verticalalignment='top',\n",
        "         fontfamily='monospace',           # Monospace for aligned text columns\n",
        "         bbox=dict(boxstyle='round,pad=0.8',\n",
        "                   facecolor='#1a1a2e',\n",
        "                   edgecolor='#7b5ea7',\n",
        "                   alpha=0.9))\n",
        "\n",
        "plt.savefig('/mnt/user-data/outputs/summary_dashboard.png',\n",
        "            dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
        "plt.close()\n",
        "print(\"  üíæ Saved: summary_dashboard.png\")\n",
        "\n",
        "# ============================================================\n",
        "#   FINAL CONSOLE SUMMARY\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"   FINAL RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n  Dataset : Handwritten Digits (sklearn)\")\n",
        "print(f\"  Samples : {total_samples}     Features: {original_features}\")\n",
        "print(f\"\\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
        "print(f\"  ‚îÇ  Variance  ‚îÇ Components ‚îÇ  Reduction  ‚îÇ  MSE   ‚îÇ\")\n",
        "print(f\"  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
        "print(f\"  ‚îÇ    90%     ‚îÇ     {n_components_90:2d}     ‚îÇ   {reduction_pct_90:5.1f}%   ‚îÇ {mse_90:.4f} ‚îÇ\")\n",
        "print(f\"  ‚îÇ    95%     ‚îÇ     {n_components_95:2d}     ‚îÇ   {reduction_pct_95:5.1f}%   ‚îÇ {mse_95:.4f} ‚îÇ\")\n",
        "print(f\"  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
        "print(f\"\\n  2D Projection Variance: {total_2d_var:.1f}%\")\n",
        "print(f\"  (PC1={var_pc1:.1f}%, PC2={var_pc2:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"   CONCLUSION\")\n",
        "print(\"=\" * 60)\n",
        "conclusion = \"\"\"\n",
        "This experiment demonstrates the effectiveness of PCA for\n",
        "dimensionality reduction on the handwritten digits dataset.\n",
        "\n",
        "The original 64-pixel features can be reduced to just\n",
        "{n90} components while retaining 90% of variance, and\n",
        "{n95} components for 95% ‚Äî reductions of {r90:.1f}% and\n",
        "{r95:.1f}% respectively. This dramatically reduces storage\n",
        "and computation while preserving most information.\n",
        "\n",
        "Reconstruction error (MSE) confirms this tradeoff: the\n",
        "90% case has higher error ({m90:.4f}) while the 95% case\n",
        "achieves lower error ({m95:.4f}) with only a few more\n",
        "components. Visual inspection shows 95% reconstructions\n",
        "are nearly indistinguishable from originals.\n",
        "\n",
        "The 2D scatter plot reveals meaningful clustering ‚Äî digit\n",
        "classes form loose but distinct regions in PC1-PC2 space,\n",
        "confirming that PCA captures true structure in the data\n",
        "even without using class labels (unsupervised). Digits\n",
        "like 0 and 1 are more isolated, while 3, 5, 8 show more\n",
        "overlap due to similar stroke patterns.\n",
        "\n",
        "Overall, PCA proves to be a powerful, interpretable tool\n",
        "for compressing high-dimensional image data with minimal\n",
        "information loss and clear geometric insight.\n",
        "\"\"\".format(n90=n_components_90, n95=n_components_95,\n",
        "           r90=reduction_pct_90, r95=reduction_pct_95,\n",
        "           m90=mse_90, m95=mse_95)\n",
        "print(conclusion)\n",
        "print(\"  ALL PLOTS SAVED SUCCESSFULLY ‚úÖ\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ]
}